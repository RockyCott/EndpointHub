# Matem√°ticas del Procesamiento de Texto y B√∫squeda Sem√°ntica

Detalle los fundamentos matem√°ticos y computacionales del procesamiento de texto para tareas de b√∫squeda sem√°ntica.

---

## ¬øQu√© significa comparar textos?

Dado un conjunto de frases y una consulta, queremos encontrar cu√°les frases son "m√°s parecidas" a esa consulta. Esto requiere:

1. Representar frases como objetos matem√°ticos (vectores, matrices, etc.).
2. Definir una forma de comparar esas representaciones.
3. Optimizar el proceso de b√∫squeda para ser r√°pido, incluso con miles o millones de frases.

Entonces es transformar texto libre en vectores num√©ricos que capturen su significado sem√°ntico para realizar comparaciones y b√∫squedas eficientes.

---

## Preprocesamiento del texto

Antes de comparar frases, se realiza una limpieza para reducir el ruido y homogenizar el contenido:

- **Min√∫sculas**: "Usuario" ‚Üí "usuario"
- **Remover puntuaci√≥n**: "¬øCrear usuario?" ‚Üí "crear usuario"
- **Tokenizaci√≥n**: "crear usuario" ‚Üí `["crear", "usuario"]`
- **Stopwords**: Eliminar palabras como "el", "la", "de"
- **Lematizaci√≥n o stemming** (opcional): "creando" ‚Üí "crear"

Esto permite que frases con diferentes formas gramaticales pero igual significado se comparen de forma m√°s precisa.

### Tokenizaci√≥n

Consiste en dividir un texto en unidades llamadas *tokens*, que usualmente son palabras, pero pueden ser subpalabras, caracteres o n-gramas.

Ejemplo:

Texto: `"Crear nuevo usuario"`
Tokens: `["Crear", "nuevo", "usuario"]`

#### Tipos comunes

- **Espaciado**: dividir por espacios
- **Basado en reglas**: eliminar signos de puntuaci√≥n, acentos
- **Subword (WordPiece, BPE)**: usado en modelos como BERT

### Limpieza y Preprocesamiento

Importancia:

- Reduce ruido e inconsistencias
- Mejora la generalizaci√≥n del modelo

Operaciones t√≠picas:

1. Min√∫sculas
2. Eliminar puntuaci√≥n
3. Normalizar palabras (lemmatizaci√≥n, stemming)
4. Quitar palabras vac√≠as (*stopwords*)
5. Expandir contracciones ("it's" ‚Üí "it is")
6. Separar CamelCase

---

## Representaci√≥n de texto: Embeddings

Un *embedding* convierte texto en un vector de n√∫meros reales en $‚Ñù^n$.

### Definici√≥n

Dado un texto $T$, su embedding es:

$\text{Embed}(T) = \vec{v} \in \mathbb{R}^d$

### Prop√≥sito

- Palabras/Frases similares tienen vectores cercanos
- Captura el contexto sem√°ntico

### M√©todos comunes

- Bag of Words (BoW)
- TF-IDF
- Word2Vec / GloVe
- Sentence Transformers (BERT, MiniLM, etc.)

Ejemplo:

- `"crear usuario"` ‚Üí $\vec{v}_1 \in \mathbb{R}^{384}$
- `"registrar nuevo cliente"` ‚Üí $\vec{v}_2 \in \mathbb{R}^{384}$

Si son similares, entonces:
$\text{distancia}(\vec{v}_1, \vec{v}_2) \approx 0$

### 1. **BoW (Bag of Words)**

Convertimos una frase en un vector basado en la frecuencia de palabras:

```txt
Frase A: "crear usuario"
Frase B: "eliminar usuario"
```

Vocabulario: `["crear", "eliminar", "usuario"]`

| Frase | crear | eliminar | usuario |
| ----- | ----- | -------- | ------- |
| A     | 1     | 0        | 1       |
| B     | 0     | 1        | 1       |

Cada fila es un vector. Para comparar, usamos una m√©trica de similitud.


### 2. **TF-IDF (Term Frequency - Inverse Document Frequency)**

Pondera palabras seg√∫n su frecuencia local y global:

$$
\text{tf-idf}(t, d) = tf(t, d) \cdot \log \left(\frac{N}{df(t)}\right)
$$

- `tf(t, d)`: frecuencia de t√©rmino `t` en documento `d`
- `df(t)`: n√∫mero de documentos que contienen el t√©rmino `t`
- `N`: total de documentos

Esto reduce el peso de palabras comunes y aumenta el de palabras distintivas.

---

## üìè Comparaci√≥n entre vectores de texto

### 1. **Distancia Euclidiana (L2)**

$$
\text{dist}(\vec{a}, \vec{b}) = \sqrt{\sum_{i=1}^{n} (a_i - b_i)^2}
$$

Cuanto menor, m√°s parecidos. √ötil para vectores densos.

### 2. **Distancia Coseno (Cosine Similarity)**

$$
\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \cdot \|\vec{b}\|}
$$

Mide el √°ngulo entre vectores, no su magnitud. Ideal para BoW o TF-IDF:

- 1 ‚Üí mismos vectores (m√°xima similitud)
- 0 ‚Üí ortogonales (sin relaci√≥n)
- -1 ‚Üí opuestos (muy raramente usado en NLP)

### Ejemplo

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

texts = ["crear usuario", "eliminar usuario"]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)
sim = cosine_similarity(X[0:1], X[1:2])
```

---

## ¬øQu√© son los embeddings?

Los embeddings son vectores densos que representan el significado de una frase. Se entrenan con modelos de aprendizaje profundo (transformers, LSTM, etc.) usando contextos para capturar relaciones sem√°nticas.

- Dimensiones t√≠picas: 384, 512, 768...
- Los modelos modernos (como BERT, MiniLM) generan estos vectores.
- Se pueden comparar con las mismas m√©tricas: L2 o coseno.

> Ejemplo: "crear usuario" y "registrar cuenta" tendr√°n embeddings cercanos.

---

## üîç B√∫squeda Sem√°ntica

Resumen del proceso:

1. Limpiar y tokenizar el texto.
2. Representar como vector (TF-IDF, BoW, embedding).
3. Comparar con vectores del corpus.
4. Retornar los m√°s similares (seg√∫n distancia).

Esto es la base matem√°tica, independientemente del modelo o librer√≠a que se use. Si bien las herramientas modernas simplifican los pasos, conocer estas bases permite evaluar, adaptar y optimizar mejor los sistemas de b√∫squeda sem√°ntica.
